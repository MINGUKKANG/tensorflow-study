{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Tensor(\"max_pool2/MaxPool:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Learniong_start\n",
      "Epoch 0001 cost =  0.437233279\n",
      "Epoch 0002 cost =  0.136431192\n",
      "Epoch 0003 cost =  0.087710854\n",
      "Epoch 0004 cost =  0.073526577\n",
      "Epoch 0005 cost =  0.064116628\n",
      "Epoch 0006 cost =  0.055085787\n",
      "Epoch 0007 cost =  0.045929476\n",
      "Epoch 0008 cost =  0.040533386\n",
      "Epoch 0009 cost =  0.036128035\n",
      "Epoch 0010 cost =  0.033898749\n",
      "Epoch 0011 cost =  0.032144378\n",
      "Epoch 0012 cost =  0.036268223\n",
      "Epoch 0013 cost =  0.029159856\n",
      "Epoch 0014 cost =  0.025061537\n",
      "Epoch 0015 cost =  0.022351041\n",
      "Learning_finish\n",
      "Accuracy: [0.98720002]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None,784], name = 'input_data')\n",
    "\n",
    "Y = tf.placeholder(tf.float32, shape = [None,10], name = 'Label_data')\n",
    "\n",
    "# for CNN, we should input 4rank data\n",
    "X_img = tf.reshape(X,[-1,28,28,1])\n",
    "\n",
    "# parameta\n",
    "batch_size = 100\n",
    "training_epoch = 15\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "display_step = 1\n",
    "logs_path = './logs1'\n",
    "\n",
    "# first convolution layers with 32 filters whose shape are [2,2,1]\n",
    "# i will use Adam Optimizer and xavier initializer\n",
    "with tf.name_scope('conv1'):\n",
    "    conv_w1 = tf.get_variable(\"conv_w1\", shape = [2,2,1,32], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    layer1 = tf.nn.conv2d(X_img, conv_w1, strides = [1,1,1,1], padding = \"SAME\")\n",
    "\n",
    "# first relu layer\n",
    "with tf.name_scope(\"relu1\"):\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "# tensor layer1's shape = [-1,28,28,32]\n",
    "\n",
    "# first max_pooling layer\n",
    "# i will make out tensor layer1 dimidiate\n",
    "with tf.name_scope(\"max_pool1\"):\n",
    "    layer1 = tf.nn.max_pool(layer1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = \"SAME\")\n",
    "\n",
    "# Second convolution layers with 64 filters whose shape are [2,2,1]\n",
    "# i also use Adam Optimizer and xavier initializer\n",
    "\n",
    "with tf.name_scope(\"conv2\"):\n",
    "    conv_w2 = tf.get_variable(\"conv_w2\", shape = [2,2,32,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    layer2 = tf.nn.conv2d(layer1,conv_w2,strides = [1,2,2,1], padding =\"SAME\")\n",
    "    # the layer2's shape is [-1,7,7,64]. you might see this, if you use print(layer2)\n",
    "\n",
    "# second relu layer\n",
    "with tf.name_scope(\"relu2\"):\n",
    "    tf.nn.relu(layer2)\n",
    "\n",
    "# second max_pooling layer\n",
    "# i will make layer2's shape same\n",
    "with tf.name_scope(\"max_pool2\"):\n",
    "    layer2 = tf.nn.max_pool(layer2, ksize = [1,1,1,1], strides = [1,1,1,1], padding = \"SAME\")\n",
    "\n",
    "print(layer2)\n",
    "\n",
    "# layer2's shape is [-1,7,7,64] and for fully connected layer it is necessary to shape layer2 [?,7*7*64]\n",
    "#7*7*64 = 3136\n",
    "with tf.name_scope(\"Reshape\"):\n",
    "    layer2 = tf.reshape(layer2, [-1,3136])\n",
    "\n",
    "# in fully connected layer, i will use AdamOptimizer, xavier initializer and dropout \n",
    "# first fully connected layer, 3136 number of input data, 1024 number of output data\n",
    "with tf.name_scope(\"first_fully\"):\n",
    "    W1 = tf.get_variable(\"W1\", shape = [3136,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.Variable(tf.random_normal([1024]), name = 'Bias_1')\n",
    "    L1 = tf.nn.relu(tf.matmul(layer2,W1)+b1)\n",
    "    L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "# second fully connected layer, 1024 number of input data, 1024 number of output data\n",
    "with tf.name_scope(\"second_fully\"):\n",
    "    W2 = tf.get_variable(\"W2\", shape = [1024,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.Variable(tf.random_normal([1024]), name = 'Bias_2')\n",
    "    L2 = tf.nn.relu(tf.matmul(L1,W2)+b2)\n",
    "    L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "# third fully connected layer, 1024 number of input data, 1024 number of output data\n",
    "with tf.name_scope(\"third_fully\"):\n",
    "    W3 = tf.get_variable(\"W3\", shape = [1024,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.Variable(tf.random_normal([1024]), name = 'Bias_3')\n",
    "    L3 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
    "    L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "# forth fully connected layer, 1024 number of input data, 512 number of output data\n",
    "with tf.name_scope(\"forth_fully\"):\n",
    "    W4 = tf.get_variable(\"W4\", shape = [1024,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.Variable(tf.random_normal([512]), name = 'Bias_4')\n",
    "    L4 = tf.nn.relu(tf.matmul(L3,W4)+b4)\n",
    "    L4 = tf.nn.dropout(L4,keep_prob = keep_prob)\n",
    "    \n",
    "# final fully connected layer, 512 number of input data, 10 number of output data\n",
    "with tf.name_scope(\"final_fully\"):\n",
    "    W5 = tf.get_variable(\"W5\", shape = [512,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.Variable(tf.random_normal([10]), name = 'Bias_5')\n",
    "    hypothesis = tf.matmul(L4,W5)+b5\n",
    "    \n",
    "# cost function with cross_entropy tensorflow function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))\n",
    "\n",
    "# AdamOptimizer code\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
    "\n",
    "with tf.name_scope(\"Accuracy_part\"):\n",
    "    is_correct = tf.equal(tf.arg_max(hypothesis,1), tf.arg_max(Y,1))\n",
    "    Accuracy = tf.reduce_mean(tf.cast(is_correct, dtype = tf.float32))\n",
    "\n",
    "\n",
    "#create a summary to monitor cost value\n",
    "tf.summary.scalar(\"cost\",cost)\n",
    "#create a summary to monitor Accuracy value\n",
    "tf.summary.scalar(\"Accuracy\", Accuracy)\n",
    "\n",
    "#mege all summaries\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "sess = tf.Session()\n",
    "#initialize all variables\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "# training cycle\n",
    "print(\"Learniong_start\")\n",
    "for epoch in range(training_epoch):\n",
    "    cost_val = 0\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size) # define number of total batch\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size) # define training data set \n",
    "        feed_dict = {X:batch_xs, Y:batch_ys, keep_prob: 0.7}  # define feed_dict and i will use drop out rate 0.7\n",
    "        c,summary,_ = sess.run([cost,merged_summary,optimizer], feed_dict = feed_dict)\n",
    "        summary_writer.add_summary(summary, epoch*total_batch+i) # epoch*total_batch + i is global step\n",
    "        cost_val += c/ total_batch \n",
    "    if (epoch +1) % display_step ==0:\n",
    "        print(\"Epoch\", '%04d' % (epoch+1), \"cost = \", \"{:.9f}\".format(cost_val))\n",
    "print(\"Learning_finish\")\n",
    "\n",
    "Accuracy_val = sess.run([Accuracy], feed_dict = {X:mnist.test.images, Y:mnist.test.labels, keep_prob:1})\n",
    "print(\"Accuracy:\", Accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
